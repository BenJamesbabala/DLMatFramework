{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cascade experiment\n",
    "Idea:\n",
    "Substitute first layer with cascade of convs.\n",
    "Example\n",
    "CONV1(5x5)-->Relu = CONV(3x3)-->CONV(3x3)-->Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MNIST data\n",
    "The MNIST data is split into three parts: 55,000 data points of training data (mnist.train), 10,000 points of test data (mnist.test), and 5,000 points of validation data (mnist.validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST training data\n",
    "each sample on the MNIST dataset consist on a 784 feature vector encoding a 28x28 grayscale image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8ANBEX4gqP8HZSqoe0qwCdEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efce7b5b8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "imgDigit = mnist.train.images[1]\n",
    "plt.imshow(imgDigit.reshape([28,28]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Force to see just the first GPU\n",
    "# https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create functions for defining conv layers\n",
    "One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients. Since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias to avoid \"dead neurons\". Instead of doing this repeatedly while we build the model, let's create two handy functions to do it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_no_relu(x, k_h, k_w, channels_in, channels_out, name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        # Define weights\n",
    "        w = tf.Variable(tf.truncated_normal([k_h,k_w, channels_in, channels_out], stddev=0.1), name=\"weights\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[channels_out]), name=\"bias\")    \n",
    "        # Convolution\n",
    "        conv = tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')    \n",
    "        \n",
    "        # Add summaries for helping debug\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"bias\", b)\n",
    "        tf.summary.histogram(\"activation\", conv)\n",
    "        return conv\n",
    "    \n",
    "def conv2d(x, k_h, k_w, channels_in, channels_out, name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        # Define weights\n",
    "        w = tf.Variable(tf.truncated_normal([k_h,k_w, channels_in, channels_out], stddev=0.1), name=\"weights\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[channels_out]), name=\"bias\")    \n",
    "        # Convolution\n",
    "        conv = tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')    \n",
    "        # Relu\n",
    "        activation = tf.nn.relu(conv + b)\n",
    "        # Add summaries for helping debug\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"bias\", b)\n",
    "        tf.summary.histogram(\"activation\", activation)\n",
    "        \n",
    "        # Display trained weights as images on tensorboard\n",
    "        # to tf.image_summary format [batch_size, height, width, channels]\n",
    "        #w_t = tf.transpose (w, [3, 0, 1, 2])\n",
    "        # this will display random 3 filters from the 64 in conv1\n",
    "        #tf.summary.image(name+'/filters', w_t[:,:,:,0], max_outputs=3)        \n",
    "\n",
    "        return activation\n",
    "\n",
    "def max_pool(x, k_h, k_w, S, name=\"maxpool\"):\n",
    "    with tf.name_scope(name):\n",
    "        return tf.nn.max_pool(x, ksize=[1, k_h, k_w, 1],strides=[1, S, S, 1], padding='SAME')\n",
    "\n",
    "def fc_layer(x, channels_in, channels_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([channels_in, channels_out], stddev=0.1), name=\"weights\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[channels_out]), name=\"bias\")    \n",
    "        activation = tf.nn.relu(tf.matmul(x, w) + b)\n",
    "        # Add summaries for helping debug\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"bias\", b)\n",
    "        tf.summary.histogram(\"activation\", activation)\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create placeholders (I/O for our model/graph)\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10], name=\"y\")\n",
    "\n",
    "# Convert the feature vector to a (-1)x28x28x1 image\n",
    "# The -1 has the same effect as the \"None\" value, and will\n",
    "# be used to inform a variable batch size\n",
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create network\n",
    "conv1 = conv2d(x_image, 5, 5, 1, 16,\"conv1\")\n",
    "\n",
    "pool1 = max_pool(conv1, 2, 2, 2, \"pool1\")\n",
    "conv2 = conv2d(pool1, 3, 3, 16, 32, \"conv2\")\n",
    "pool2 = max_pool(conv2, 2, 2, 2, \"pool2\")\n",
    "# Needs calculation... (-1 means any batch size)\n",
    "flattened = tf.reshape(pool2, [-1, 7*7*32])\n",
    "fc1 = fc_layer(flattened, 7*7*32, 1024, \"fc1\")\n",
    "# Add dropout to the fully connected layer\n",
    "fc1_drop = tf.nn.dropout(fc1, 0.5)\n",
    "scores = fc_layer(fc1_drop, 1024, 10, \"fc2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cascading version no RELU in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create network\n",
    "#conv1 = conv2d(x_image, 5, 5, 1, 16,\"conv1\") #25x16=400 Weights\n",
    "# 9x32+9x16=432\n",
    "conv1_1 = conv2d_no_relu(x_image, 3, 3, 1, 32,\"conv1_1\")\n",
    "conv1_2 = conv2d(conv1_1, 3, 3, 32, 16,\"conv1_2\")\n",
    "\n",
    "pool1 = max_pool(conv1_2, 2, 2, 2, \"pool1\")\n",
    "conv2 = conv2d(pool1, 3, 3, 16, 32, \"conv2\")\n",
    "pool2 = max_pool(conv2, 2, 2, 2, \"pool2\")\n",
    "# Needs calculation... (-1 means any batch size)\n",
    "flattened = tf.reshape(pool2, [-1, 7*7*32])\n",
    "fc1 = fc_layer(flattened, 7*7*32, 1024, \"fc1\")\n",
    "# Add dropout to the fully connected layer\n",
    "fc1_drop = tf.nn.dropout(fc1, 0.5)\n",
    "scores = fc_layer(fc1_drop, 1024, 10, \"fc2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cascading version no RELU same filter-size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create network\n",
    "#conv1 = conv2d(x_image, 5, 5, 1, 16,\"conv1\") #25x16=400 Weights\n",
    "# 9x32+9x16=432\n",
    "conv1_1 = conv2d_no_relu(x_image, 3, 3, 1, 16,\"conv1_1\")\n",
    "conv1_2 = conv2d(conv1_1, 3, 3, 16, 16,\"conv1_2\")\n",
    "\n",
    "pool1 = max_pool(conv1_2, 2, 2, 2, \"pool1\")\n",
    "conv2 = conv2d(pool1, 3, 3, 16, 32, \"conv2\")\n",
    "pool2 = max_pool(conv2, 2, 2, 2, \"pool2\")\n",
    "# Needs calculation... (-1 means any batch size)\n",
    "flattened = tf.reshape(pool2, [-1, 7*7*32])\n",
    "fc1 = fc_layer(flattened, 7*7*32, 1024, \"fc1\")\n",
    "# Add dropout to the fully connected layer\n",
    "fc1_drop = tf.nn.dropout(fc1, 0.5)\n",
    "scores = fc_layer(fc1_drop, 1024, 10, \"fc2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cascading version with RELU in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create network\n",
    "#conv1 = conv2d(x_image, 5, 5, 1, 16,\"conv1\") #25x16=400 Weights\n",
    "# 9x32+9x16=432\n",
    "conv1_1 = conv2d(x_image, 3, 3, 1, 32,\"conv1_1\")\n",
    "conv1_2 = conv2d(conv1_1, 3, 3, 32, 16,\"conv1_2\")\n",
    "\n",
    "pool1 = max_pool(conv1_2, 2, 2, 2, \"pool1\")\n",
    "conv2 = conv2d(pool1, 3, 3, 16, 32, \"conv2\")\n",
    "pool2 = max_pool(conv2, 2, 2, 2, \"pool2\")\n",
    "# Needs calculation... (-1 means any batch size)\n",
    "flattened = tf.reshape(pool2, [-1, 7*7*32])\n",
    "fc1 = fc_layer(flattened, 7*7*32, 1024, \"fc1\")\n",
    "# Add dropout to the fully connected layer\n",
    "fc1_drop = tf.nn.dropout(fc1, 0.5)\n",
    "scores = fc_layer(fc1_drop, 1024, 10, \"fc2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a loss function and how to optimize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiclass cross-entropy\n",
    "with tf.name_scope(\"cross_entropy\"):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=y))\n",
    "\n",
    "# Solver configuration\n",
    "with tf.name_scope(\"Solver\"):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope(\"Accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(scores,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Add summary for cross_entropy loss, accuracy and the image\n",
    "tf.summary.scalar(\"cross_entropy\", cross_entropy)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "tf.summary.image(\"input\", x_image, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build graph\n",
    "Now we will initialize our variables, and create a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Avoid allocating the whole memory\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "#sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"/tmp/mnist_demo/1\")\n",
    "writer.add_graph(sess.graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train graph (model/hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.16\n",
      "step 100, training accuracy 0.1\n",
      "step 200, training accuracy 0.26\n",
      "step 300, training accuracy 0.4\n",
      "step 400, training accuracy 0.64\n",
      "step 500, training accuracy 0.8\n",
      "step 600, training accuracy 0.9\n",
      "step 700, training accuracy 0.98\n",
      "step 800, training accuracy 0.86\n",
      "step 900, training accuracy 1\n",
      "step 1000, training accuracy 0.94\n",
      "step 1100, training accuracy 0.98\n",
      "step 1200, training accuracy 0.94\n",
      "step 1300, training accuracy 0.96\n",
      "step 1400, training accuracy 0.98\n",
      "step 1500, training accuracy 0.96\n",
      "step 1600, training accuracy 0.98\n",
      "step 1700, training accuracy 0.98\n",
      "step 1800, training accuracy 0.9\n",
      "step 1900, training accuracy 0.96\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000):\n",
    "    # Get batch of 50 images\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    \n",
    "    # Dump summary\n",
    "    if i % 5 == 0:\n",
    "        s = sess.run(merged_summary, feed_dict={x:batch[0], y: batch[1]})\n",
    "        writer.add_summary(s,i)    \n",
    "\n",
    "    # Print accuracy\n",
    "    # Print each 100 epochs\n",
    "    if i%100 == 0:\n",
    "        # Calculate train accuracy\n",
    "        train_accuracy = accuracy.eval(session=sess, feed_dict={x:batch[0], y: batch[1]})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "                \n",
    "    \n",
    "    # Train actually here\n",
    "    #train_step.run(session=sess, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating hypothesis\n",
    "To display information on tensorboard use\n",
    "```bash\n",
    "tensorboard --logdir /tmp/mnist_demo/1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9608\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "On the MNIST dataset the only thing that we can observe is that cascading layers make the convergence slower.\n",
    "#### TODO:\n",
    "Try on Resnet-18 with our dataset"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
